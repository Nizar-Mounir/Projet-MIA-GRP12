{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a utilisé plusieurs type de classifier qui vont être presentés ici (mais on a opté a utilisé XGBoost car c'est celui qui a donné les meilleurs résultats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "in_file_template = \"data/shadowTrainResults/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-50,extra_checkpoint_freq-50,run-0,sample_len-7,self_norm-False,shadow_id-{i},/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    "synthetic_file = \"/home/azerty/snake2-beta-insa-main/data/publicData/syntheticTask1.npz\"\n",
    "model_file = \"saved_model/linear_svc_model.joblib\"\n",
    "\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "print(\"--- Debug ---\")\n",
    "try:\n",
    "    print(f\"Loading synthetic data file: {synthetic_file}\")\n",
    "    synthetic_data = np.load(synthetic_file)[\"data_feature\"].reshape((-1, 7))\n",
    "    print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Add synthetic data with label 1\n",
    "    data_list.append(synthetic_data)\n",
    "    labels_list.append(np.ones(synthetic_data.shape[0], dtype=int))  # Label all synthetic data as 1\n",
    "\n",
    "    for i in range(1, 41):\n",
    "        in_file = in_file_template.format(i=i)\n",
    "        print(f\"Loading in_file: {in_file}\")\n",
    "        \n",
    "        shadow_data_generated = np.load(in_file)[\"data_feature\"].reshape((-1, 7))\n",
    "        print(f\"shadow_data_generated shape (i={i}): {shadow_data_generated.shape}\")\n",
    "\n",
    "        # Compute distances between shadow-generated data and synthetic data\n",
    "        distances = cdist(shadow_data_generated, synthetic_data, metric=\"euclidean\")\n",
    "        print(f\"Computed distances shape (i={i}): {distances.shape}\")\n",
    "\n",
    "        # Find minimum distance for each shadow data row\n",
    "        min_distances = distances.min(axis=1)\n",
    "        print(f\"Minimum distances shape (i={i}): {min_distances.shape}\")\n",
    "\n",
    "        # Assign labels based on threshold\n",
    "        close_indices = min_distances <= THRESHOLD\n",
    "        far_indices = ~close_indices\n",
    "\n",
    "        # Append nearby shadow data (label 1) and far shadow data (label 0)\n",
    "        data_list.append(shadow_data_generated[close_indices])\n",
    "        labels_list.append(np.ones(close_indices.sum(), dtype=int))  # Label as 1\n",
    "        data_list.append(shadow_data_generated[far_indices])\n",
    "        labels_list.append(np.zeros(far_indices.sum(), dtype=int))  # Label as 0\n",
    "\n",
    "    combined_data = np.vstack(data_list)\n",
    "    combined_labels = np.hstack(labels_list)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Combined labels shape: {combined_labels.shape}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.3, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during debugging:\", e)\n",
    "    raise\n",
    "\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "\n",
    "classifier = LinearSVC(random_state=42, max_iter=10000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(classifier, model_file)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "in_file_template = \"data/shadowTrainResults/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-50,extra_checkpoint_freq-50,run-0,sample_len-7,self_norm-False,shadow_id-{i},/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    "synthetic_file = \"/home/azerty/snake2-beta-insa-main/data/publicData/syntheticTask1.npz\"\n",
    "model_file = \"saved_model/logistic_regression_model.joblib\"\n",
    "\n",
    "THRESHOLD = 0.17\n",
    "\n",
    "print(\"--- Debug ---\")\n",
    "try:\n",
    "    print(f\"Loading synthetic data file: {synthetic_file}\")\n",
    "    synthetic_data = np.load(synthetic_file)[\"data_feature\"].reshape((-1, 7))\n",
    "    print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "    \n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Add synthetic data with label 1\n",
    "    data_list.append(synthetic_data)\n",
    "    labels_list.append(np.ones(synthetic_data.shape[0], dtype=int))  # Label all synthetic data as 1\n",
    "\n",
    "    for i in range(1, 41):\n",
    "        in_file = in_file_template.format(i=i)\n",
    "        print(f\"Loading in_file: {in_file}\")\n",
    "        \n",
    "        shadow_data_generated = np.load(in_file)[\"data_feature\"].reshape((-1, 7))\n",
    "        print(f\"shadow_data_generated shape (i={i}): {shadow_data_generated.shape}\")\n",
    "\n",
    "        # Compute distances between shadow-generated data and synthetic data\n",
    "        distances = cdist(shadow_data_generated, synthetic_data, metric=\"cityblock\")\n",
    "        print(f\"Computed distances shape (i={i}): {distances.shape}\")\n",
    "\n",
    "        # Find minimum distance for each shadow data row\n",
    "        min_distances = distances.min(axis=1)\n",
    "        print(f\"Minimum distances shape (i={i}): {min_distances.shape}\")\n",
    "\n",
    "        # Assign labels based on threshold\n",
    "        close_indices = min_distances <= THRESHOLD\n",
    "        far_indices = ~close_indices\n",
    "\n",
    "        # Append nearby shadow data (label 1) and far shadow data (label 0)\n",
    "        data_list.append(shadow_data_generated[close_indices])\n",
    "        labels_list.append(np.ones(close_indices.sum(), dtype=int))  # Label as 1\n",
    "        data_list.append(shadow_data_generated[far_indices])\n",
    "        labels_list.append(np.zeros(far_indices.sum(), dtype=int))  # Label as 0\n",
    "\n",
    "    combined_data = np.vstack(data_list)\n",
    "    combined_labels = np.hstack(labels_list)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Combined labels shape: {combined_labels.shape}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.2, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during debugging:\", e)\n",
    "    raise\n",
    "\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "\n",
    "log_reg_classifier = LogisticRegression(\n",
    "    solver='lbfgs',       # Optimizer algorithm (lbfgs is recommended for small datasets)\n",
    "    max_iter=1000,        # Maximum number of iterations for optimization\n",
    "    random_state=42       # Random state for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Training the Logistic Regression classifier...\")\n",
    "log_reg_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Saving the model to {model_file}\")\n",
    "joblib.dump(log_reg_classifier, model_file)\n",
    "\n",
    "y_pred = log_reg_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "in_file_template = \"data/shadowTrainResults/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-50,extra_checkpoint_freq-50,run-0,sample_len-7,self_norm-False,shadow_id-{i},/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    "synthetic_file = \"/home/azerty/snake2-beta-insa-main/data/publicData/syntheticTask1.npz\"\n",
    "model_file = \"saved_model/rf_classifier_model.joblib\"\n",
    "\n",
    "THRESHOLD = 0.18\n",
    "\n",
    "print(\"--- Debug ---\")\n",
    "try:\n",
    "    print(f\"Loading synthetic data file: {synthetic_file}\")\n",
    "    synthetic_data = np.load(synthetic_file)[\"data_feature\"].reshape((-1, 7))\n",
    "    print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Add synthetic data with label 1\n",
    "    data_list.append(synthetic_data)\n",
    "    labels_list.append(np.ones(synthetic_data.shape[0], dtype=int))  # Label all synthetic data as 1\n",
    "\n",
    "    for i in range(1, 41):\n",
    "        in_file = in_file_template.format(i=i)\n",
    "        print(f\"Loading in_file: {in_file}\")\n",
    "        \n",
    "        shadow_data_generated = np.load(in_file)[\"data_feature\"].reshape((-1, 7))\n",
    "        print(f\"shadow_data_generated shape (i={i}): {shadow_data_generated.shape}\")\n",
    "\n",
    "        # Compute distances between shadow-generated data and synthetic data\n",
    "        distances = cdist(shadow_data_generated, synthetic_data, metric=\"cityblock\")\n",
    "        print(f\"Computed distances shape (i={i}): {distances.shape}\")\n",
    "\n",
    "        # Find minimum distance for each shadow data row\n",
    "        min_distances = distances.min(axis=1)\n",
    "        print(f\"Minimum distances shape (i={i}): {min_distances.shape}\")\n",
    "\n",
    "        # Assign labels based on threshold\n",
    "        close_indices = min_distances <= THRESHOLD\n",
    "        far_indices = ~close_indices\n",
    "\n",
    "        # Append nearby shadow data (label 1) and far shadow data (label 0)\n",
    "        data_list.append(shadow_data_generated[close_indices])\n",
    "        labels_list.append(np.ones(close_indices.sum(), dtype=int))  # Label as 1\n",
    "        data_list.append(shadow_data_generated[far_indices])\n",
    "        labels_list.append(np.zeros(far_indices.sum(), dtype=int))  # Label as 0\n",
    "\n",
    "    combined_data = np.vstack(data_list)\n",
    "    combined_labels = np.hstack(labels_list)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Combined labels shape: {combined_labels.shape}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.3, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during debugging:\", e)\n",
    "    raise\n",
    "\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Training the Random Forest classifier...\")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Saving the model to {model_file}\")\n",
    "joblib.dump(rf_classifier, model_file)\n",
    "\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "in_file_template = \"data/shadowTrainResults/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-50,extra_checkpoint_freq-50,run-0,sample_len-7,self_norm-False,shadow_id-{i},/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    "synthetic_file = \"/home/azerty/snake2-beta-insa-main/data/publicData/syntheticTask1.npz\"\n",
    "model_file = \"saved_model/lgbm_classifier_model.joblib\"\n",
    "\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "print(\"--- Debug ---\")\n",
    "try:\n",
    "    print(f\"Loading synthetic data file: {synthetic_file}\")\n",
    "    synthetic_data = np.load(synthetic_file)[\"data_feature\"].reshape((-1, 7))\n",
    "    print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Add synthetic data with label 1\n",
    "    data_list.append(synthetic_data)\n",
    "    labels_list.append(np.ones(synthetic_data.shape[0], dtype=int))  # Label all synthetic data as 1\n",
    "\n",
    "    for i in range(1, 41):\n",
    "        in_file = in_file_template.format(i=i)\n",
    "        print(f\"Loading in_file: {in_file}\")\n",
    "        \n",
    "        shadow_data_generated = np.load(in_file)[\"data_feature\"].reshape((-1, 7))\n",
    "        print(f\"shadow_data_generated shape (i={i}): {shadow_data_generated.shape}\")\n",
    "\n",
    "        # Compute distances between shadow-generated data and synthetic data\n",
    "        distances = cdist(shadow_data_generated, synthetic_data, metric=\"cityblock\")\n",
    "        print(f\"Computed distances shape (i={i}): {distances.shape}\")\n",
    "\n",
    "        # Find minimum distance for each shadow data row\n",
    "        min_distances = distances.min(axis=1)\n",
    "        print(f\"Minimum distances shape (i={i}): {min_distances.shape}\")\n",
    "\n",
    "        # Assign labels based on threshold\n",
    "        close_indices = min_distances <= THRESHOLD\n",
    "        far_indices = ~close_indices\n",
    "\n",
    "        # Append nearby shadow data (label 1) and far shadow data (label 0)\n",
    "        data_list.append(shadow_data_generated[close_indices])\n",
    "        labels_list.append(np.ones(close_indices.sum(), dtype=int))  # Label as 1\n",
    "        data_list.append(shadow_data_generated[far_indices])\n",
    "        labels_list.append(np.zeros(far_indices.sum(), dtype=int))  # Label as 0\n",
    "\n",
    "    combined_data = np.vstack(data_list)\n",
    "    combined_labels = np.hstack(labels_list)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Combined labels shape: {combined_labels.shape}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.3, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during debugging:\", e)\n",
    "    raise\n",
    "\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "\n",
    "lgbm_classifier = lgb.LGBMClassifier(\n",
    "    n_estimators=100,         # Number of boosting iterations\n",
    "    max_depth=6,              # Maximum depth of each tree\n",
    "    learning_rate=0.1,        # Learning rate\n",
    "    subsample=0.8,            # Subsample ratio for training instances\n",
    "    colsample_bytree=0.8,     # Subsample ratio of features\n",
    "    random_state=42           # Random state for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Training the LightGBM classifier...\")\n",
    "lgbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Saving the model to {model_file}\")\n",
    "joblib.dump(lgbm_classifier, model_file)\n",
    "\n",
    "y_pred = lgbm_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
