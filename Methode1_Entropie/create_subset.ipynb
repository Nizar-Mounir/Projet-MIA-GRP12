{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code est utilisé pour crées les sous-ensembles d'entrainement des shadow model basée sur la méthode Entropie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "from toDopelganger import normalize, privateToDopel\n",
    "\n",
    "# Load the dataset using pandas\n",
    "input_file = snakemake.input[0]\n",
    "dataset = pd.read_parquet(input_file)\n",
    "\n",
    "# Define parameters for splitting\n",
    "num_subsets = snakemake.params.num_subsets  # Number of subsets\n",
    "base_output_directory = \"data/shadowData34\"\n",
    "\n",
    "# Function to calculate entropy for a column\n",
    "def calculate_entropy(column):\n",
    "    value_counts = column.value_counts(normalize=True)\n",
    "    return entropy(value_counts, base=2)\n",
    "\n",
    "# Calculate entropy for all columns\n",
    "entropy_scores = dataset.apply(calculate_entropy)\n",
    "\n",
    "# Add a column for the sum of entropy scores across relevant columns\n",
    "dataset[\"combined_entropy\"] = dataset[entropy_scores.index].sum(axis=1)\n",
    "\n",
    "# Sort the dataset by combined entropy scores (descending order)\n",
    "sorted_dataset = dataset.sort_values(by=\"combined_entropy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Split the sorted dataset into subsets in a round-robin fashion\n",
    "subsets = [sorted_dataset.iloc[i::num_subsets] for i in range(num_subsets)]\n",
    "\n",
    "# Process and save each subset\n",
    "for i, subset in enumerate(subsets, start=1):  # Start numbering from 1\n",
    "    # Define the folder name for this subset\n",
    "    shadow_folder = os.path.join(base_output_directory, f\"shadow{i}\")\n",
    "    \n",
    "    # Create the shadow folder if it doesn't exist\n",
    "    os.makedirs(shadow_folder, exist_ok=True)\n",
    "    \n",
    "    # Drop the helper column before further processing\n",
    "    subset = subset.drop(columns=[\"combined_entropy\"])\n",
    "\n",
    "    # Convert subset to NumPy array\n",
    "    subset_array = subset.to_numpy()\n",
    "\n",
    "    # Define the file path\n",
    "    file_name = \"data_train.npz\"\n",
    "    file_path = os.path.join(shadow_folder, file_name)\n",
    "    \n",
    "    # Save the subset as .npz file\n",
    "    np.savez(file_path, data=subset_array)\n",
    "\n",
    "    # Normalize and process the subset for DoppelGANger\n",
    "    genFlags = ~np.isnan(subset_array)  # Generate flags for missing data\n",
    "    subset_array = np.nan_to_num(subset_array, nan=0.0)  # Replace NaNs with 0s\n",
    "    subset_array = normalize(subset_array)  # Normalize the subset\n",
    "\n",
    "    # Save the data in DoppelGANger format\n",
    "    privateToDopel(subset_array, genFlags, shadow_folder)\n",
    "\n",
    "    # Optionally, print progress for each subset\n",
    "    print(f\"Processed and saved subset {i} in {shadow_folder}\")\n",
    "\n",
    "print(f\"Data subsets saved in {base_output_directory}/shadow*/\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
