{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost classifier (Task 1 et Task2) , on change seulement les path des données d'entrainement générées, synthétiques issues du données privées et des targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "in_file_template = \"/home/azerty/snake2-beta-insa-main/snake2-beta-insa-main/data/shadowTrainResults/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-50,extra_checkpoint_freq-50,run-0,sample_len-7,self_norm-False,shadow_id-{i},/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    "synthetic_file = \"/home/azerty/snake2-beta-insa-main/snake2-beta-insa-main/data/publicData/syntheticTask1.npz\"\n",
    "model_file = \"saved_model/xgb_classifier_model2.joblib\"\n",
    "\n",
    "THRESHOLD = 0.08\n",
    "\n",
    "print(\"--- Debug ---\")\n",
    "try:\n",
    "    print(f\"Loading synthetic data file: {synthetic_file}\")\n",
    "    synthetic_data = np.load(synthetic_file)[\"data_feature\"].reshape((-1, 7))\n",
    "    print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Add synthetic data with label 1\n",
    "    data_list.append(synthetic_data)\n",
    "    labels_list.append(np.ones(synthetic_data.shape[0], dtype=int))  # Label all synthetic data as 1\n",
    "\n",
    "    for i in range(1, 41): \n",
    "        in_file = in_file_template.format(i=i)\n",
    "        print(f\"Loading in_file: {in_file}\")\n",
    "        \n",
    "        shadow_data_generated = np.load(in_file)[\"data_feature\"].reshape((-1, 7))\n",
    "        print(f\"shadow_data_generated shape (i={i}): {shadow_data_generated.shape}\")\n",
    "\n",
    "        # Compute distances between shadow-generated data and synthetic data\n",
    "        distances = cdist(shadow_data_generated, synthetic_data, metric=\"euclidean\")\n",
    "        print(f\"Computed distances shape (i={i}): {distances.shape}\")\n",
    "\n",
    "        # Find minimum distance for each shadow data row\n",
    "        min_distances = distances.min(axis=1)\n",
    "        print(f\"Minimum distances shape (i={i}): {min_distances.shape}\")\n",
    "\n",
    "        # Assign labels based on threshold\n",
    "        close_indices = min_distances <= THRESHOLD\n",
    "        far_indices = ~close_indices\n",
    "\n",
    "        # Append nearby shadow data (label 1) and far shadow data (label 0)\n",
    "        data_list.append(shadow_data_generated[close_indices])\n",
    "        labels_list.append(np.ones(close_indices.sum(), dtype=int))  # Label as 1\n",
    "        data_list.append(shadow_data_generated[far_indices])\n",
    "        labels_list.append(np.zeros(far_indices.sum(), dtype=int))  # Label as 0\n",
    "\n",
    "    # Combine data and labels from all sources\n",
    "    combined_data = np.vstack(data_list)\n",
    "    combined_labels = np.hstack(labels_list)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Combined labels shape: {combined_labels.shape}\")\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.3, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during debugging:\", e)\n",
    "    raise\n",
    "\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "\n",
    "xgb_classifier = XGBClassifier(\n",
    "    n_estimators=100,         # Number of trees\n",
    "    max_depth=6,              # Maximum depth of each tree\n",
    "    learning_rate=0.1,        # Learning rate for optimization\n",
    "    subsample=0.8,            # Subsample ratio for training instances\n",
    "    colsample_bytree=0.8,     # Subsample ratio of features for each tree\n",
    "    random_state=42           # Random state for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Training the XGBoost classifier...\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Saving the model to {model_file}\")\n",
    "joblib.dump(xgb_classifier, model_file)\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "csv_file = \"/home/azerty/snake2-beta-insa-main/snake2-beta-insa-main/data/publicData/targetsTask4.csv\"\n",
    "model_file = \"saved_model/xgb_classifier_model2.joblib\"\n",
    "\n",
    "\n",
    "print(\"--- Predicting Labels ---\")\n",
    "try:\n",
    "    print(f\"Loading model from file: {model_file}\")\n",
    "    classifier = joblib.load(model_file)\n",
    "\n",
    "    print(f\"Loading CSV data from file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    test_data = df.iloc[:, 2:].values # here we take all columns of targets\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "    predicted_labels = classifier.predict(test_data)\n",
    "    print(f\"Predicted labels for test data: {predicted_labels}\")\n",
    "\n",
    "    output_file = \"/home/azerty/snake2-beta-insa-main/snake2-beta-insa-main/data/predicted_labels_test.npy\"\n",
    "    np.save(output_file, predicted_labels)\n",
    "    print(f\"Predicted labels saved to {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during prediction:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost classifier (Task 3 et Task4) , on change seulement les path des données d'entrainement générées, synthétiques issues du données privées et des targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "# File paths\n",
    "in_file_template = \"/home/azerty/snake2-beta-insa-main/snake2-beta-insa-main/data/shadow34/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-50,extra_checkpoint_freq-50,run-0,sample_len-7,self_norm-False,shadow_id-{i},/generated_samples3/epoch_id-999/generated_data_train.npz\"\n",
    "synthetic_file = \"/home/azerty/snake2-beta-insa-main/snake2-beta-insa-main/data/publicData/syntheticTask4.npz\"\n",
    "model_file = \"saved_model/xgb_classifier_model4.joblib\"\n",
    "\n",
    "# Threshold for Manhattan distance\n",
    "THRESHOLD = 0.02\n",
    "\n",
    "# Set NumPy to display full precision for debugging\n",
    "np.set_printoptions(precision=17, suppress=False)\n",
    "\n",
    "print(\"--- Debug ---\")\n",
    "try:\n",
    "    # Load synthetic data from the target model\n",
    "    print(f\"Loading synthetic data file: {synthetic_file}\")\n",
    "    synthetic_data = np.load(synthetic_file)[\"data_feature\"].astype(np.float64).reshape((-1, 7))\n",
    "    print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "\n",
    "    # Select columns 1:4 from synthetic data\n",
    "    synthetic_data_subset = synthetic_data[:, 1:4]\n",
    "    print(f\"synthetic_data_subset shape: {synthetic_data_subset.shape}\")\n",
    "\n",
    "    # Initialize lists for data and labels\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Add synthetic data subset with label 1\n",
    "    data_list.append(synthetic_data_subset)\n",
    "    labels_list.append(np.ones(synthetic_data_subset.shape[0], dtype=int))  # Label all synthetic data as 1\n",
    "\n",
    "    # Iterate through shadow models\n",
    "    for i in range(1, 27):  # shadow1 to shadow40\n",
    "        in_file = in_file_template.format(i=i)\n",
    "        print(f\"Loading in_file: {in_file}\")\n",
    "        \n",
    "        shadow_data_generated = np.load(in_file)[\"data_feature\"].astype(np.float64).reshape((-1, 7))\n",
    "        print(f\"shadow_data_generated shape (i={i}): {shadow_data_generated.shape}\")\n",
    "\n",
    "        # Select columns 4:7 from shadow data\n",
    "        shadow_data_subset = shadow_data_generated[:, 4:7]\n",
    "        print(f\"shadow_data_subset shape (i={i}): {shadow_data_subset.shape}\")\n",
    "\n",
    "        # Compute Manhattan distances between shadow-generated data and synthetic data\n",
    "        distances = cdist(shadow_data_subset, synthetic_data_subset, metric=\"euclidean\")\n",
    "        print(f\"Computed distances shape (i={i}): {distances.shape}\")\n",
    "\n",
    "        # Find minimum distance for each shadow data row\n",
    "        min_distances = distances.min(axis=1)\n",
    "        print(f\"Minimum distances shape (i={i}): {min_distances.shape}\")\n",
    "        print(f\"Example min_distances (i={i}): {min_distances[:10]}\")\n",
    "\n",
    "        # Assign labels based on threshold\n",
    "        close_indices = min_distances <= THRESHOLD\n",
    "        far_indices = ~close_indices\n",
    "\n",
    "        # Append nearby shadow data (label 1) and far shadow data (label 0)\n",
    "        data_list.append(shadow_data_subset[close_indices])\n",
    "        labels_list.append(np.ones(close_indices.sum(), dtype=int))  # Label as 1\n",
    "        data_list.append(shadow_data_subset[far_indices])\n",
    "        labels_list.append(np.zeros(far_indices.sum(), dtype=int))  # Label as 0\n",
    "\n",
    "    # Combine data and labels from all sources\n",
    "    combined_data = np.vstack(data_list)\n",
    "    combined_labels = np.hstack(labels_list)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Combined labels shape: {combined_labels.shape}\")\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.3, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during debugging:\", e)\n",
    "    raise\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(\n",
    "    n_estimators=100,         # Number of trees\n",
    "    max_depth=6,              # Maximum depth of each tree\n",
    "    learning_rate=0.1,        # Learning rate for optimization\n",
    "    subsample=0.8,            # Subsample ratio for training instances\n",
    "    colsample_bytree=0.8,     # Subsample ratio of features for each tree\n",
    "    random_state=42           # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Train the XGBoost classifier\n",
    "print(\"Training the XGBoost classifier...\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "print(f\"Saving the model to {model_file}\")\n",
    "joblib.dump(xgb_classifier, model_file)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
