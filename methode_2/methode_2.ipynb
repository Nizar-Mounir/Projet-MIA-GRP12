{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deuxième méthode\n",
    "\n",
    "### Splitting des dataset par distance Manhattan\n",
    "Les données les plus proches des targets parmi le dataset publique seront découpé en subdatasets de 10000 lignes pour l'entraineemnt de chaque shadow model (en tout 26).\n",
    "\n",
    "La création d'un public_dataset_index va nous permettre de retrouver les lignes membres dans le dataset d'entrainement des shadow models à la dernière étape de notre méthode.\n",
    "\n",
    "Le résultat seras dans `data/shadowDataFIN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"scripts\"))\n",
    "from toDopelganger import normalize, privateToDopel\n",
    "\n",
    "# Load datasets\n",
    "public_dataset = pd.read_parquet(\"../data/publicData/publicDatasetTask1-2.parquet\")\n",
    "target_dataset = pd.read_csv(\"../data/publicData/targetsTask1.csv\")\n",
    "\n",
    "# Add a public_dataset_index column with line numbers\n",
    "public_dataset['public_dataset_index'] = range(len(public_dataset))\n",
    "\n",
    "# Extract numerical columns from both datasets\n",
    "public_data = public_dataset.drop(columns=['public_dataset_index'])  # Exclude index from calculations\n",
    "target_data = target_dataset.iloc[:, 2:]  # Skip metadata columns\n",
    "\n",
    "# Define parameters\n",
    "base_output_directory = \"../data/shadowDataFIN\"\n",
    "distance_metric = \"cityblock\"  # Change to \"cityblock\" for Manhattan distance\n",
    "max_distance = 0.4  # Maximum allowed distance \n",
    "rows_per_subset = 10000  # Number of rows per subset\n",
    "\n",
    "\n",
    "# Step 1: Calculate distances\n",
    "\n",
    "public_array = public_data.to_numpy()\n",
    "target_array = target_data.to_numpy()\n",
    "\n",
    "# Compute distances between each row of public and target datasets\n",
    "distances = cdist(public_array, target_array, metric=distance_metric)\n",
    "\n",
    "# Calculate the minimum distance for each row in the public dataset\n",
    "min_distances = distances.min(axis=1)\n",
    "\n",
    "# Add the minimum distance as a new column in the public dataset\n",
    "public_dataset[\"min_distance\"] = min_distances\n",
    "\n",
    "# Step 2: Filter rows based on max_distance\n",
    "filtered_dataset = public_dataset[public_dataset[\"min_distance\"] <= max_distance]\n",
    "\n",
    "# Step 3: Randomize the rows after filtering\n",
    "shuffled_dataset = filtered_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Split dataset into subsets of 10,000 rows each\n",
    "subsets = [shuffled_dataset.iloc[i:i + rows_per_subset] for i in range(0, len(shuffled_dataset), rows_per_subset)]\n",
    "\n",
    "# Step 5: Process and save subsets for training the shadow models\n",
    "for i, subset in enumerate(subsets, start=1): \n",
    "    # Define folder name for the subset\n",
    "    shadow_folder = os.path.join(base_output_directory, f\"shadow{i}\")\n",
    "    os.makedirs(shadow_folder, exist_ok=True)\n",
    "\n",
    "    # Drop the helper column before further processing\n",
    "    subset = subset.drop(columns=[\"min_distance\"])\n",
    "\n",
    "    # Extract index from subset and create subset_array for DopelGanger\n",
    "    subset_with_index = subset[['public_dataset_index']].to_numpy(dtype=np.int64)\n",
    "    subset_array = subset.drop(columns=['public_dataset_index']).to_numpy(dtype=np.float64)  \n",
    "\n",
    "    # Define the file path\n",
    "    file_name = \"data_train.npz\"\n",
    "    file_path = os.path.join(shadow_folder, file_name)\n",
    "\n",
    "    # Save the subset as .npz file (without index)\n",
    "    np.savez(file_path, data=subset_array)\n",
    "\n",
    "    # Normalize and process the subset for DopelGANger\n",
    "    genFlags = ~np.isnan(subset_array)  # Generate flags for missing data\n",
    "    subset_array = np.nan_to_num(subset_array, nan=0.0)  # Replace NaNs with 0s\n",
    "    subset_array = normalize(subset_array)  # Normalize the subset\n",
    "\n",
    "    # Save the data in DopelGANger format\n",
    "    privateToDopel(subset_array, genFlags, shadow_folder)\n",
    "\n",
    "    # Create a CSV for each subset, including the public_dataset_index and line index\n",
    "    csv_file_path = os.path.join(shadow_folder, \"data_train.csv\")\n",
    "    \n",
    "    # Add public_dataset_index as the first column\n",
    "    csv_data = np.hstack((np.arange(len(subset))[:, np.newaxis], subset_with_index, subset_array))\n",
    "    csv_columns = ['index', 'public_dataset_index'] + [f\"{j}\" for j in range(subset_array.shape[1])]\n",
    "\n",
    "    pd.DataFrame(csv_data, columns=csv_columns).to_csv(csv_file_path, index=False)\n",
    "    print(f\"Processed and saved subset {i} in {shadow_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement des shadows models et génération de leur données sythétiques\n",
    "\n",
    "Pour entrainer les shadow models sur leurs datasets respectifs il faut lancer le snakefile avec la commande \n",
    "\n",
    "`$ conda activate snakemake` et après `$ snakemake -c all -R all --use-conda` pour lancer les rules dans le snakefile \n",
    "\n",
    "Le résultat seras dans `data/shadowtrainResultsFIN`\n",
    "\n",
    "### Entrainement du classifieur et classification des synthétiques privés\n",
    "\n",
    "#### Classifier SVC Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# File paths\n",
    "train_file_template = (\n",
    "    \"../data/shadowTrainResultsFIN/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-100,\"\n",
    "    \"extra_checkpoint_freq-100,run-0,sample_len-7,self_norm-False,shadow_id-{i},\"\n",
    "    \"/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    ")\n",
    "test_file_template = (\n",
    "    \"../data/shadowTrainResultsFIN/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-100,\"\n",
    "    \"extra_checkpoint_freq-100,run-0,sample_len-7,self_norm-False,shadow_id-{i},\"\n",
    "    \"/generated_samples/epoch_id-999/generated_data_test.npz\"\n",
    ")\n",
    "\n",
    "# private synthetic to be classified\n",
    "synthetic_task_file = \"../data/publicData/syntheticTask1.npz\"\n",
    "\n",
    "# placeholder of the model\n",
    "model_file = \"../saved_model/linear_svc_model_telescop.joblib\"\n",
    "\n",
    "## lines for debugging\n",
    "## print(\"--- Debug ---\")\n",
    "try:\n",
    "    train_data_list = []\n",
    "    train_labels_list = []\n",
    "    test_data_list = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    for i in range(1, 27):  # Shadow IDs from 1 to 26 (the number of our splitted datasets)\n",
    "        # Load training data\n",
    "        train_file = train_file_template.format(i=i)\n",
    "        # lines for debugging\n",
    "        #print(f\"Loading train_file: {train_file}\")\n",
    "        train_data = np.load(train_file)[\"data_feature\"]\n",
    "        train_data = train_data.reshape((train_data.shape[0], train_data.shape[1]))[:, :7]\n",
    "        train_data_list.append(train_data)\n",
    "        train_labels_list.append(np.full((train_data.shape[0],), i))  # Shadow ID as label\n",
    "\n",
    "        # Load test data\n",
    "        test_file = test_file_template.format(i=i)\n",
    "        # lines for debugging\n",
    "        #print(f\"Loading test_file: {test_file}\")\n",
    "        test_data = np.load(test_file)[\"data_feature\"]\n",
    "        test_data = test_data.reshape((test_data.shape[0], test_data.shape[1]))[:, :7]\n",
    "        test_data_list.append(test_data)\n",
    "        test_labels_list.append(np.full((test_data.shape[0],), i))  # Shadow ID as label\n",
    "\n",
    "    # Combine data and labels\n",
    "    train_data_combined = np.vstack(train_data_list)\n",
    "    train_labels_combined = np.hstack(train_labels_list)\n",
    "    print(f\"Combined train_data shape: {train_data_combined.shape}\")\n",
    "    print(f\"Combined train_labels shape: {train_labels_combined.shape}\")\n",
    "\n",
    "    test_data_combined = np.vstack(test_data_list)\n",
    "    test_labels_combined = np.hstack(test_labels_list)\n",
    "    print(f\"Combined test_data shape: {test_data_combined.shape}\")\n",
    "    print(f\"Combined test_labels shape: {test_labels_combined.shape}\")\n",
    "except Exception as e:\n",
    "    print(\"Error during data loading:\", e)\n",
    "    raise\n",
    "\n",
    "# training the classifier\n",
    "classifier = LinearSVC(random_state=42, max_iter=10000)\n",
    "classifier.fit(train_data_combined,train_labels_combined)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(classifier, model_file)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(test_data_combined)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy on test data:\", accuracy_score(test_labels_combined, y_pred))\n",
    "print(\"Classification Report on test data:\\n\", classification_report(test_labels_combined, y_pred, zero_division=0))\n",
    "\n",
    "def classify_time_series(file_path):\n",
    "    \"\"\"\n",
    "    Classifies the time series data in the given file.\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing time series data.\n",
    "    Returns:\n",
    "        np.ndarray: Predicted shadow IDs for the time series.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading time series data from: {file_path}\")\n",
    "        synthetic_data = np.load(file_path)[\"data_feature\"]\n",
    "\n",
    "        # Reshape and trim to 7 columns to \n",
    "        synthetic_data = synthetic_data.reshape((synthetic_data.shape[0], synthetic_data.shape[1]))[:, :7]\n",
    "        print(f\"synthetic_data shape: {synthetic_data.shape}\")\n",
    "\n",
    "        predictions = classifier.predict(synthetic_data)\n",
    "        print(\"Predictions for the synthetic task file:\", predictions)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        print(\"Error during time series classification:\", e)\n",
    "        return None\n",
    "\n",
    "# Using the classify_time_series function\n",
    "try:\n",
    "    predictions = classify_time_series(synthetic_task_file)\n",
    "    print(\"Predictions for synthetic task file:\", predictions)\n",
    "except Exception as e:\n",
    "    print(\"Error during classification of synthetic task file:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier K-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# File paths\n",
    "train_file_template = (\n",
    "    \"../data/shadowTrainResultsFIN/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-100,\"\n",
    "    \"extra_checkpoint_freq-100,run-0,sample_len-7,self_norm-False,shadow_id-{i},\"\n",
    "    \"/generated_samples/epoch_id-999/generated_data_train.npz\"\n",
    ")\n",
    "test_file_template = (\n",
    "    \"../data/shadowTrainResultsFIN/aux_disc-False,dataset-shadow{i},epoch-1000,epoch_checkpoint_freq-100,\"\n",
    "    \"extra_checkpoint_freq-100,run-0,sample_len-7,self_norm-False,shadow_id-{i},\"\n",
    "    \"/generated_samples/epoch_id-999/generated_data_test.npz\"\n",
    ")\n",
    "\n",
    "# private synthetic to be classified \n",
    "synthetic_task_file = \"../data/publicData/syntheticTask1.npz\"\n",
    "\n",
    "# placeholder of the model\n",
    "model_file = \"../saved_model/knn_model_telescop.joblib\"\n",
    "\n",
    "# line for debugging\n",
    "#print(\"--- Debug ---\")\n",
    "try:\n",
    "    train_data_list = []\n",
    "    train_labels_list = []\n",
    "    test_data_list = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    for i in range(1, 27):  # Shadow IDs from 1 to 27\n",
    "        # Load training data\n",
    "        train_file = train_file_template.format(i=i)\n",
    "        # lines for debugging\n",
    "        #print(f\"Loading train_file: {train_file}\")\n",
    "        train_data = np.load(train_file)[\"data_feature\"]\n",
    "        train_data = train_data.reshape((train_data.shape[0], train_data.shape[1]))[:, :7]\n",
    "        train_data_list.append(train_data)\n",
    "        train_labels_list.append(np.full((train_data.shape[0],), i))  # Shadow ID as label\n",
    "\n",
    "        # Load test data\n",
    "        test_file = test_file_template.format(i=i)\n",
    "        # lines for debugging\n",
    "        #print(f\"Loading test_file: {test_file}\")\n",
    "        test_data = np.load(test_file)[\"data_feature\"]\n",
    "        test_data = test_data.reshape((test_data.shape[0], test_data.shape[1]))[:, :7]\n",
    "        test_data_list.append(test_data)\n",
    "        test_labels_list.append(np.full((test_data.shape[0],), i))  # Shadow ID as label\n",
    "\n",
    "    # Combine data and labels\n",
    "    train_data_combined = np.vstack(train_data_list)\n",
    "    train_labels_combined = np.hstack(train_labels_list)\n",
    "    print(f\"Combined train_data shape: {train_data_combined.shape}\")\n",
    "    print(f\"Combined train_labels shape: {train_labels_combined.shape}\")\n",
    "\n",
    "    test_data_combined = np.vstack(test_data_list)\n",
    "    test_labels_combined = np.hstack(test_labels_list)\n",
    "    print(f\"Combined test_data shape: {test_data_combined.shape}\")\n",
    "    print(f\"Combined test_labels shape: {test_labels_combined.shape}\")\n",
    "except Exception as e:\n",
    "    print(\"Error during data loading:\", e)\n",
    "    raise\n",
    "\n",
    "n_neighbors = 5 \n",
    "classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "classifier.fit(train_data_combined, train_labels_combined)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(classifier, model_file)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(test_data_combined)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy on test data:\", accuracy_score(test_labels_combined, y_pred))\n",
    "print(\"Classification Report on test data:\\n\", classification_report(test_labels_combined, y_pred, zero_division=0))\n",
    "\n",
    "# Using the classify_time_series function\n",
    "try:\n",
    "    predictions = classify_time_series(synthetic_task_file)\n",
    "    print(\"Predictions for synthetic task file:\", predictions)\n",
    "except Exception as e:\n",
    "    print(\"Error during classification of synthetic task file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder of the model\n",
    "model_file = \"../saved_model/knn_model_telescop_h-8.joblib\"\n",
    "\n",
    "n_neighbors = 40\n",
    "classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "classifier.fit(train_data_combined, train_labels_combined)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(classifier, model_file)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(test_data_combined)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy on test data:\", accuracy_score(test_labels_combined, y_pred))\n",
    "print(\"Classification Report on test data:\\n\", classification_report(test_labels_combined, y_pred, zero_division=0))\n",
    "\n",
    "# Using the classify_time_series function\n",
    "try:\n",
    "    predictions = classify_time_series(synthetic_task_file)\n",
    "    print(\"Predictions for synthetic task file:\", predictions)\n",
    "except Exception as e:\n",
    "    print(\"Error during classification of synthetic task file:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model_file = \"../saved_model/xgboost_model.joblib\"\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "classifier = xgb.XGBClassifier(\n",
    "    objective=\"multi:softmax\",  # Multi-class classification\n",
    "    num_class=54,              # Number of classes\n",
    "    max_depth=6,               # Maximum depth of trees\n",
    "    learning_rate=0.1,         # Learning rate\n",
    "    n_estimators=100,          # Number of trees\n",
    "    use_label_encoder=False    # Suppress label encoding warning\n",
    ")\n",
    "train_labels_combined -= 1\n",
    "test_labels_combined -= 1\n",
    "# Train the XGBoost classifier\n",
    "classifier.fit(train_data_combined, train_labels_combined)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(classifier, model_file)\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(test_data_combined)\n",
    "\n",
    "# Using the classify_time_series function\n",
    "try:\n",
    "    predictions = classify_time_series(synthetic_task_file)\n",
    "    print(\"Predictions for synthetic task file:\", predictions)\n",
    "except Exception as e:\n",
    "    print(\"Error during classification of synthetic task file:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification des données synthétiques\n",
    "\n",
    "Classification des données synthétiques des membres en prenant les données synthétiques privé les plus proches des données targets.\n",
    "\n",
    "On utilise le k-neighbour classifier par sa présision plus haute que les autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load \n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Paths to input files\n",
    "synthetic_task_file = \"../data/publicData/syntheticTask1.npz\"\n",
    "model_file = \"../saved_model/knn_model_telescop_h-8.joblib\"\n",
    "targets_task_file = \"../data/publicData/targetsTask1.csv\"\n",
    "shadow_data_dir_template = \"../data/shadowDataFIN/shadow{i}/data_train.csv\"\n",
    "output_vector_file = \"../data/output_binary_vector.txt\"\n",
    "\n",
    "# Distance threshold\n",
    "THRESHOLD = 0.1\n",
    "\n",
    "# Load the classifier\n",
    "classifier = load(model_file)\n",
    "\n",
    "# Load synthetic task data\n",
    "synthetic_task_data = np.load(synthetic_task_file)[\"data_feature\"]\n",
    "synthetic_task_data = synthetic_task_data.reshape((synthetic_task_data.shape[0], synthetic_task_data.shape[1]))[:, :7] #Reshape and trim to 7 columns to \n",
    "\n",
    "# Load the targets dataset\n",
    "targets_df = pd.read_csv(targets_task_file)\n",
    "targets_data = targets_df.iloc[:, 2:]  # Skip metadata columns\n",
    "\n",
    "# Compute distances between synthetic data and target data\n",
    "distances = cdist(synthetic_task_data, targets_data, metric=\"cityblock\")\n",
    "print(f\"Computed distances shape: {distances.shape}\")\n",
    "\n",
    "# Find synthetic points that are within the threshold\n",
    "min_distances = distances.min(axis=1)\n",
    "close_indices = np.where(min_distances <= THRESHOLD)[0]\n",
    "print(f\"Number of synthetic points close to targets data: {len(close_indices)}\")\n",
    "\n",
    "# Filter synthetic task data to classify only close points\n",
    "filtered_synthetic_data = synthetic_task_data[close_indices]\n",
    "\n",
    "# Classify the filtered synthetic task data\n",
    "try:\n",
    "    predictions = classifier.predict(filtered_synthetic_data)\n",
    "except Exception as e:\n",
    "    print(\"Error during classification of filtered synthetic task data:\", e)\n",
    "    predictions = None\n",
    "\n",
    "if predictions is not None:\n",
    "    # Step 2: Identify the labels by percentage\n",
    "    label_counts = Counter(predictions)\n",
    "    total_count = sum(label_counts.values())\n",
    "    label_percentages = {label: (count / total_count) * 100 for label, count in label_counts.items()}\n",
    "    sorted_labels_by_percentage = sorted(label_percentages, key=label_percentages.get, reverse=True)\n",
    "    print(\"Top labels by percentage:\", sorted_labels_by_percentage)\n",
    "\n",
    "    # Step 3: Load target indices\n",
    "    targets_df = pd.read_csv(targets_task_file)\n",
    "    targets_indices = targets_df['index']\n",
    "\n",
    "    # Step 4: Create the binary vector\n",
    "    binary_vector = np.zeros(len(targets_indices), dtype=int)\n",
    "\n",
    "    # Convert targets_indices to a set for faster lookup\n",
    "    target_set = set(targets_indices)\n",
    "\n",
    "    # Only loop through shadow datasets corresponding to the classifier's predictions\n",
    "    predicted_labels = set(predictions)\n",
    "\n",
    "    for label in sorted_labels_by_percentage:  # Loop only over predicted shadow datasets\n",
    "        try:\n",
    "            shadow_data_path = shadow_data_dir_template.format(i=label)\n",
    "            shadow_df = pd.read_csv(shadow_data_path)\n",
    "\n",
    "            # Check which shadow indices are in the target set\n",
    "            shadow_indices = shadow_df['public_dataset_index']\n",
    "            matching_indices = shadow_indices[shadow_indices.isin(target_set)]\n",
    "\n",
    "            # Mark matches in the binary vector\n",
    "            for match in matching_indices:\n",
    "                binary_vector[targets_indices[targets_indices == match].index[0]] = 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing shadow dataset {label}: {e}\")\n",
    "\n",
    "    # Step 5: Save the binary vector to a text file\n",
    "    with open(output_vector_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(map(str, binary_vector)))\n",
    "\n",
    "    print(f\"Binary vector saved to {output_vector_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score de la labelisation des targets\n",
    "\n",
    "On score la précision de notre attaque en utilisant la formule utilisé dans la compétition ( obtenue dans le fichier `CodabenchBundle/scoringProgram/score.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "# File paths\n",
    "guessFile = \"../data/output_binary_vector.txt\"\n",
    "truthFile = \"../data/privateData/truthTask1.txt\"\n",
    "\n",
    "truth=np.loadtxt(truthFile)\n",
    "assert os.path.isfile(guessFile)\n",
    "guesses=np.loadtxt(guessFile)\n",
    "assert truth.shape==guesses.shape\n",
    "assert np.bitwise_or(guesses==1, guesses==0).all() \n",
    "\n",
    "cm = confusion_matrix(truth, guesses)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (tn + fp)\n",
    "ma = tpr - fpr\n",
    "ma = (ma + 1) / 2\n",
    "\n",
    "print(f\"computed score {ma}\")\n",
    "res={\"ma\":ma}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakemake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
